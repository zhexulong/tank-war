# 专家策略学习（Expert Policy Learning）

本项目实现了基于专家策略的强化学习方法，通过模仿专家行为来加速RL智能体的学习过程。

## 原理介绍

专家策略学习（也称为模仿学习或行为克隆）是一种结合专家知识与强化学习的混合学习方法。在这种方法中：

1. 系统中有一个"专家智能体"（本项目中为LogicExpertAgent），它具有一定的领域知识和策略
2. RL智能体在学习过程中，除了通过环境反馈获得奖励外，还会因为其动作与专家一致而获得额外奖励
3. 随着训练的进展，专家奖励逐渐减少，鼓励RL智能体逐渐发展自己的策略

### 动作空间映射

在本项目中，RL智能体和专家智能体的动作空间不完全一致：

- **RL智能体动作空间**：0-停留, 1-前进, 2-后退, 3-左转, 4-右转, 5-开火 (6个动作)
- **专家智能体动作空间**：0-停留, 1-前进, 2-左转, 3-右转, 4-开火 (5个动作，无后退)

为了正确比较动作，系统在比较时会对RL智能体的动作进行映射：
- 停留和前进保持不变
- 后退动作视为不一致（专家不使用后退）
- 左转、右转和开火的索引根据动作空间差异进行调整

这种方法对于加速学习早期阶段特别有效，可以帮助RL智能体更快地掌握基本策略，避免长时间的随机探索。

## 关键参数

- `use_expert`：是否启用专家策略学习
- `expert_reward_init`：初始专家奖励值，决定了专家指导的强度
- `expert_decay_factor`：专家奖励衰减因子
  - `1.0`：线性衰减，随训练进度从初始值降到0
  - `0.0`：不衰减，始终保持初始奖励值
  - 其他值：调整衰减速度

## 使用方法

### 命令行参数

```bash
# 使用默认专家策略学习（线性衰减）
python train_script.py --use-expert --expert-reward 1.0 --expert-decay 1.0

# 使用不衰减的专家策略学习（强依赖专家）
python train_script.py --use-expert --expert-reward 0.5 --expert-decay 0.0

# 不使用专家策略学习（纯RL）
python train_script.py --no-expert
```

### 示例脚本

项目提供了`run_expert_learning.py`脚本，演示不同专家策略学习配置：

```bash
python run_expert_learning.py
```

## 性能评估

训练过程中会记录并绘制以下指标：

1. 奖励曲线
2. 损失曲线
3. 胜率曲线
4. 游戏步数曲线
5. 专家一致率曲线（使用专家策略时）

通过比较专家一致率与胜率的变化，可以观察RL智能体从模仿专家到发展自己策略的过程。

## 最佳实践

1. **早期强指导，后期弱指导**：设置较高的初始奖励和适当的衰减因子
2. **避免过度依赖**：确保专家奖励最终能降到很低或为零
3. **结合专家一致率分析**：如果一致率持续很高但性能不佳，可能表明专家策略并非最优
4. **调整衰减速度**：如果智能体学习缓慢，可以减小衰减因子，延长指导期
